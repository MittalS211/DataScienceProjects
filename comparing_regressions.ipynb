{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/markespina/Downloads/kickstarter-projects/ks-projects-201801.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creature binary outcome variable\n",
    "df['outcome'] = df['usd_pledged_real'] >= df['usd_goal_real']\n",
    "\n",
    "# recode bool to numeric\n",
    "df['outcome'] = np.where(df['outcome'] == True, 1, 0)\n",
    "df['country'] = np.where(df['country'] == 'US', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out categorical features\n",
    "features = [f for f in df.columns if df[f].dtype != 'object']\n",
    "\n",
    "#remove redudant/unecessary features\n",
    "features.remove('goal')\n",
    "features.remove('pledged')\n",
    "features.remove('ID')\n",
    "features.remove('usd pledged')\n",
    "features.remove('usd_pledged_real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['backers', 'country', 'usd_goal_real', 'outcome']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creature additional feature sets\n",
    "categories = pd.get_dummies(df.main_category)\n",
    "more_features = pd.concat([df[features], categories], axis=1)\n",
    "\n",
    "even_more_features = pd.get_dummies(df.category)\n",
    "even_more_features = pd.concat([df[features], even_more_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set X, y and additional feature sets\n",
    "\n",
    "y = df[features.pop()]\n",
    "X = df[features]\n",
    "\n",
    "\n",
    "X2 = more_features.drop('outcome', axis=1)\n",
    "X3 = even_more_features.drop('outcome', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients\n",
      "[[ 0.03680844 -0.00032972 -0.00022858]]\n",
      "[-0.00047613]\n",
      "\n",
      " Accuracy by project outcome\n",
      "outcome      0      1\n",
      "row_0                \n",
      "0        54665   2999\n",
      "1         5758  31244\n",
      "\n",
      " Percentage accuracy\n",
      "0.9074958274354045\n",
      "[0.90758344 0.90927334 0.90874525 0.9065075  0.90724699 0.9100993\n",
      " 0.90988802 0.90312698 0.90988802 0.90502852]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "logr = LogisticRegression()\n",
    "fit = logr.fit(X_train, y_train)\n",
    "print('Coefficients')\n",
    "print(fit.coef_)\n",
    "print(fit.intercept_)\n",
    "pred_y_sklearn = logr.predict(X_test)\n",
    "\n",
    "print('\\n Accuracy by project outcome')\n",
    "print(pd.crosstab(pred_y_sklearn, y_test))\n",
    "\n",
    "print('\\n Percentage accuracy')\n",
    "print(logr.score(X_test, y_test))\n",
    "\n",
    "print(cross_val_score(logr, X_test, y_test, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients\n",
      "[[ 0.044594   -0.0096738  -0.00026849 -0.00106148 -0.00055946 -0.00080628\n",
      "   0.00011121 -0.00107879 -0.00137504 -0.00043788 -0.00082849 -0.00256236\n",
      "  -0.00033112 -0.00056253 -0.00064473 -0.00243291 -0.00088507  0.00028423]]\n",
      "[-0.01317069]\n",
      "\n",
      " Accuracy by project outcome\n",
      "outcome      0      1\n",
      "row_0                \n",
      "0        54960   2916\n",
      "1         5371  31419\n",
      "\n",
      " Percentage accuracy\n",
      "0.912460651131346\n",
      "[0.90716096 0.90546108 0.91053132 0.91053132 0.91105947 0.91421931\n",
      " 0.91126136 0.91347982 0.90893725 0.90883161]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y)\n",
    "\n",
    "logr = LogisticRegression()\n",
    "fit = logr.fit(X_train, y_train)\n",
    "print('Coefficients')\n",
    "print(fit.coef_)\n",
    "print(fit.intercept_)\n",
    "pred_y_sklearn = logr.predict(X_test)\n",
    "\n",
    "print('\\n Accuracy by project outcome')\n",
    "print(pd.crosstab(pred_y_sklearn, y_test))\n",
    "\n",
    "print('\\n Percentage accuracy')\n",
    "print(logr.score(X_test, y_test))\n",
    "\n",
    "print(cross_val_score(logr, X_test, y_test, cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logististic regression performs pretty well with R2 over 90% and seem to be consisent over 10 folds\n",
    "Adding some additional features barely increased accuracy\n",
    "Theme seems to be a bit some class imbalance in the sample let's see if SMOTE will improve anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "smote = SMOTE(random_state=0)\n",
    "X_balanced, y_balanced = smote.fit_sample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients\n",
      "[[ 0.04544106 -0.00017497 -0.0002194 ]]\n",
      "[-0.0002953]\n",
      "\n",
      " Accuracy by project outcome\n",
      "col_0      0      1\n",
      "row_0              \n",
      "0      52515   3458\n",
      "1       7661  57176\n",
      "\n",
      " Percentage accuracy\n",
      "0.9079629169770714\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced)\n",
    "logr = LogisticRegression()\n",
    "fit = logr.fit(X_train, y_train)\n",
    "print('Coefficients')\n",
    "print(fit.coef_)\n",
    "print(fit.intercept_)\n",
    "pred_y_sklearn = logr.predict(X_test)\n",
    "\n",
    "print('\\n Accuracy by project outcome')\n",
    "print(pd.crosstab(pred_y_sklearn, y_test))\n",
    "\n",
    "print('\\n Percentage accuracy')\n",
    "print(logr.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3% original false positive rate\n",
      "0.59% original false negative rate\n",
      "0.37% SMOTE false positive rate\n",
      "0.8% SMOTE false negative rate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Using Smote, model performed worse with an increase in both type 1 and type 2 errors'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(str(round((2880/966476),4)*100) + \"% original false positive rate\")\n",
    "print(str(round((5674/966476),4)*100) + \"% original false negative rate\")\n",
    "print(str(round((3561/966476),4)*100) + \"% SMOTE false positive rate\")\n",
    "print(str(round((7711/966476),4)*100) + \"% SMOTE false negative rate\")\n",
    "\n",
    "\"\"\"Using Smote, model performed worse with an increase in both type 1 and type 2 errors\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R² for the model with few features:\n",
      "0.6198740836203072\n",
      "[ 1.22263263e-04 -2.59628400e-01 -2.15207974e-08]\n",
      "\n",
      "R² for the model with more features:\n",
      "0.6742970020915641\n",
      "[ 1.31290463e-04  8.35515614e-02 -1.85201065e-08 -2.47804223e-01\n",
      "  8.82596215e-03 -5.85463237e-01  1.61925916e-01 -3.65650036e-01\n",
      " -5.67412762e-01 -3.21278688e-01 -5.69850526e-01 -3.75389040e-01\n",
      " -6.44074875e-01 -9.35957941e-02 -4.58422503e-01 -4.52809508e-01\n",
      " -6.59356483e-01  1.35836374e-01]\n"
     ]
    }
   ],
   "source": [
    "# let's try rigde with our original feature set and added feature set\n",
    "from sklearn import linear_model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "ridge_regr = linear_model.RidgeClassifier(alpha=5, fit_intercept=False)\n",
    "\n",
    "ridge_regr.fit(X_train, y_train)\n",
    "print('\\nR² for the model with few features:')\n",
    "print(ridge_regr.score(X_test, y_test))\n",
    "origparams = ridge_regr.coef_[0]\n",
    "print(origparams)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y) # x2 has additional category columns\n",
    "ridge_regr = linear_model.RidgeClassifier(alpha=5, fit_intercept=False)\n",
    "\n",
    "ridge_regr.fit(X_train, y_train)\n",
    "print('\\nR² for the model with more features:')\n",
    "print(ridge_regr.score(X_test, y_test))\n",
    "newparams = ridge_regr.coef_[0]\n",
    "print(newparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While rigde performs far worse than logistic regression, unlike logistic regression the model exprienced a increase in predictive accuracy with the more complex model let's run it one more time with more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameter estimates for the model with many features:\n",
      "0.6858428580482961\n",
      "[ 1.30340741e-04  6.30648254e-02 -1.61219916e-08 -3.59745172e-01\n",
      " -6.16587579e-01 -3.53995156e-01 -7.32665067e-01 -4.89827378e-01\n",
      " -5.23704845e-01  2.27812758e-01 -6.47539040e-01 -9.19478427e-01\n",
      " -5.62352787e-01 -2.32290155e-01 -6.36586082e-02 -4.85127610e-01\n",
      " -6.17411486e-01 -1.25869482e-01 -2.48412537e-01 -1.47924163e-01\n",
      " -8.01408114e-01 -1.99557346e-01 -3.63939895e-01 -5.84850955e-01\n",
      "  3.37781481e-01 -3.26975614e-01  2.00536946e-01 -2.53153661e-01\n",
      "  1.09928191e-01 -6.30671745e-02 -6.15128396e-01 -3.63052115e-01\n",
      " -5.50246238e-01  2.04005142e-01 -7.41543070e-01 -5.46212641e-01\n",
      " -5.84694719e-01 -7.23408019e-01 -1.27877463e-01  2.41498097e-01\n",
      " -3.78922466e-01 -4.77244404e-01 -3.31712889e-01 -2.69426901e-01\n",
      " -5.56719250e-01 -4.08827036e-01 -6.33544884e-01 -6.63559869e-01\n",
      " -2.40606888e-01 -4.72822162e-01 -2.22168095e-01 -6.46108049e-01\n",
      " -3.64845850e-01 -6.74292745e-01 -6.18985399e-01 -5.37074322e-01\n",
      "  7.08342770e-02 -5.61147987e-01 -3.82173672e-01 -3.30168976e-01\n",
      " -6.68644177e-01 -4.77048364e-01 -8.00830855e-01 -3.82582217e-01\n",
      " -4.60253691e-01 -5.52901801e-01 -5.83847663e-01 -4.54704426e-01\n",
      " -3.13882265e-01  2.99321635e-02 -4.09724579e-01 -7.49238519e-01\n",
      " -4.17324263e-01 -1.21164427e-02 -4.81765782e-02  2.14021501e-01\n",
      " -6.71956996e-02 -6.15546851e-01  1.10471534e-01 -5.46422972e-01\n",
      " -4.93416411e-01 -1.71910449e-01 -1.15536519e-01 -4.03708421e-01\n",
      "  1.75973225e-01 -5.04917634e-02 -1.72443438e-01 -6.74249155e-01\n",
      " -3.41398255e-01 -2.72500273e-01 -3.56111610e-01 -8.55644190e-01\n",
      " -5.78476430e-01 -7.06037003e-02 -3.71917845e-01 -3.42767281e-02\n",
      " -3.00385156e-01 -6.84372014e-01 -5.12244105e-01 -3.54035710e-01\n",
      " -6.27211167e-01 -1.87212566e-01  1.50496126e-01 -2.53067831e-01\n",
      " -5.13054416e-01 -6.84068296e-01 -2.25044314e-01 -3.99999417e-01\n",
      " -6.95169819e-01 -2.72139494e-01  1.16475221e-01 -3.24899422e-01\n",
      " -1.19955713e-01 -1.13263584e-01 -5.91311970e-01 -6.17301503e-01\n",
      " -3.39162135e-01 -6.37647793e-02 -5.04280023e-01 -1.76014325e-01\n",
      " -3.00826290e-01 -6.46195202e-01 -6.09970423e-01 -2.15229947e-01\n",
      " -6.94167601e-01  3.34808501e-01 -7.31978118e-01 -2.70365866e-01\n",
      " -3.06201827e-02 -4.35386989e-01 -3.10354739e-01 -2.86494773e-01\n",
      "  2.56047815e-02 -4.26616725e-01 -7.96352291e-01 -2.27071551e-01\n",
      " -3.42938995e-01 -3.50003102e-01 -3.88661058e-01  2.80856133e-02\n",
      "  2.67172417e-02 -6.62020160e-01 -7.34269391e-01 -4.71570671e-01\n",
      "  1.83516397e-01 -4.95525050e-01 -5.64875982e-01  7.45549648e-02\n",
      " -3.63279431e-01 -7.62055266e-01 -5.34900432e-01 -6.81208721e-01\n",
      " -3.97407826e-01 -4.44303155e-01 -8.69647328e-01  2.61820790e-02\n",
      " -4.66516555e-01 -5.52850578e-01 -4.02864937e-01 -1.63083053e-01\n",
      " -6.14201289e-01 -1.06070132e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:112: LinAlgWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number5.133450e-17\n",
      "  overwrite_a=True).T\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X3, y) # x3 has even more category columns\n",
    "ridge_regr = linear_model.RidgeClassifier(alpha=10, fit_intercept=False)\n",
    "\n",
    "ridge_regr.fit(X_train, y_train)\n",
    "print('\\nParameter estimates for the model with many features:')\n",
    "print(ridge_regr.score(X_test, y_test))\n",
    "newparams = ridge_regr.coef_[0]\n",
    "print(newparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive accuracy is maxing out at 68%, lets try lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² for the model with few features:\n",
      "0.01581156564264463\n",
      "\n",
      "Parameter estimates for the model with few features:\n",
      "[ 6.23546148e-05  0.00000000e+00 -9.41714309e-09  3.55926523e-01]\n",
      "\n",
      "R² for the model with many features:\n",
      "0.017204468414754426\n",
      "\n",
      "Parameter estimates for the model with many features:\n",
      "[ 6.84554788e-05  0.00000000e+00 -1.00057328e-08  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  3.54634665e-01]\n"
     ]
    }
   ],
   "source": [
    "lass = linear_model.Lasso(alpha=.1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "lassfit = lass.fit(X_train, y_train)\n",
    "print('R² for the model with few features:')\n",
    "print(lass.score(X_train, y_train))\n",
    "origparams = np.append(lassfit.coef_, lassfit.intercept_)\n",
    "print('\\nParameter estimates for the model with few features:')\n",
    "print(origparams)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y)\n",
    "# Larger number of parameters.\n",
    "lassBig = linear_model.Lasso(alpha=.1)\n",
    "lassBig.fit(X_train, y_train)\n",
    "print('\\nR² for the model with many features:')\n",
    "print(lassBig.score(X_train, y_train))\n",
    "origparams = np.append(lassBig.coef_, lassBig.intercept_)\n",
    "print('\\nParameter estimates for the model with many features:')\n",
    "print(origparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Of the three regression models , Logistic Regression preformed the best with an accurracy rate of 90% (with the simplest feature set to boot!)\n",
    "Rigde seems to benefit the most from a heavier feature set\n",
    "Lasso Performed the worse with the lowest accuracy and little benefit from more complex feature sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
